# infancy
Building an MLP Neural Network from scratch, using only numpy. My main goal is to reduce the degree of "black-box" understanding with NNs, specifically back-propagation.

## RUNNING & TRAINING THE NETWORK

To run:
1. Clone the repo
2. Run ```python testingEnvironment.py```

## PERFORMANCE
Epochs (iterations): 400 <br />
Average loss after training: 14.65~ <br />
Accuracy: // TODO: ADD ACCURACY TESTING <br />

## DESIGN
You can read my detailed thoughts here: [Notion Page](https://scarlet-uranium-4df.notion.site/infancy-a-neural-net-from-scratch-80263381d0654c2c99e33c2c2eae582e?pvs=4) <br />

The following is a visualization of the forward propogation alg for each layer, but note that the activation functions might change <br />
![Forward propagation visualization](https://github.com/sh2002vk/infancy/assets/63932832/d9a2407e-d1df-46bb-ab29-b70f987ee2d6)
